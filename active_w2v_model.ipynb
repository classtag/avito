{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import gc\n",
    "import logging\n",
    "from pymystem3 import Mystem\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import multiprocessing as mp\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/kaggle/competitions/avito-demand-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_matrix(matrix, name):\n",
    "    pickle.dump(matrix, open(root + 'features/{}.pkl'.format(name), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "def load_matrix(name):\n",
    "    return pickle.load(open(root + 'features/{}.pkl'.format(name), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "punctuation = set(string.punctuation)\n",
    "emoji = load_matrix('emoji')\n",
    "chuncksize = 100000\n",
    "usecols = ['param_1', 'param_2', 'param_3', 'title', 'description']\n",
    "model = Word2Vec(size=100, window=5, max_vocab_size=500000)\n",
    "update = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_1 = re.compile(\"\\d+х\\d+х\\d+\")\n",
    "rule_2 = re.compile(\"\\d+х\\d+\")\n",
    "rule_3 = re.compile(\"\\d*[-|–]\\d*\")\n",
    "rule_4 = re.compile(\"\\d*\\\\.\\d*\")\n",
    "rule_5 = re.compile(\"([^\\W\\d_]+)(\\d+)\")\n",
    "rule_6 = re.compile(\"(\\d+)([^\\W\\d_]+)\")\n",
    "rule_7 = re.compile(\"\\d+\\\\/\\d|\\d+-к|\\d+к|\\\\.\\/|\\d+х\\d+х\\d+|\\d+х\\d+\")\n",
    "rule_8 = re.compile(\"\\\\s+\")\n",
    "rule_9 = re.compile(\"([nn\\\\s]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    s = rule_1.sub('nxnxn ', s)\n",
    "    s = rule_2.sub('nxn ', s)\n",
    "    s = rule_3.sub('nn ', s)\n",
    "    s = rule_4.sub('n ', s)\n",
    "    s = rule_5.sub(lambda m: 'n' + m.group(1) + ' ', s)\n",
    "    s = rule_6.sub(lambda m: 'n' + m.group(2) + ' ', s)\n",
    "    s = rule_7.sub(' ', s.lower())\n",
    "    \n",
    "    s = ''.join([c if c.isalpha() or c.isalnum() or c.isspace() else ' ' for c in s if s not in emoji and s not in punctuation and not s.isnumeric()])\n",
    "    s = rule_8.sub(' ', s)\n",
    "    s = rule_9.sub('nn ', s)\n",
    "    s = s.strip()\n",
    "    words = [w for w in s.split(' ') if w not in stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(file):\n",
    "    sentences_file = root+'features/{}_sentences.txt'.format(file)\n",
    "    print('prepare_text {}'.format(sentences_file))\n",
    "    i = 0\n",
    "    with open(sentences_file, 'w') as sf:\n",
    "        for df in pd.read_csv(root + file + '.csv.zip', chunksize=10000, usecols=usecols):\n",
    "            df['text'] = df['param_1'].str.cat([df.param_2, df.param_3, df.title, df.description], sep=' ', na_rep='')\n",
    "            sentences = df['text'].apply(normalize_text).values\n",
    "            sf.writelines(sentences)\n",
    "            i += len(sentences)\n",
    "            del sentences, df\n",
    "            gc.collect()\n",
    "            print('Write {}W text'.format(i // 10000))\n",
    "    print(file+\" complated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_w2v(sentences_file):\n",
    "    global update\n",
    "    with open(sentences_file, 'r') as sf:\n",
    "        sentences = sf.readlines(100000)\n",
    "        fit_w2v(sentences, update)\n",
    "        sentences = sentences.split()\n",
    "        model.build_vocab(sentences, update=update)\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=3)\n",
    "        update = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['train', 'test', 'train_active']\n",
    "for file in files:\n",
    "    for file in files:\n",
    "        p = mp.Process(target=prepare_text, args=(file,))\n",
    "        p.start()\n",
    "    p.join()\n",
    "    print('prepare_text completed')\n",
    "    \n",
    "    sentences_file = root+'features/{}_sentences.txt'.format(file)\n",
    "    for k in range(10):\n",
    "        print(20 * '=' + 'Epoch {}, File {}'.format(k, file) + 20 * '=')\n",
    "        fit_w2v(sentences_file)\n",
    "    print(30 * '=' + '{} train finished'.format(file) + '=' * 30)\n",
    "    model.save(root+'/features/avito.w2v')\n",
    "    print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
