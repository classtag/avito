{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "from utils import *\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, os, nltk, string, re\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_obj tr_index started\n",
      "load_obj tr_index: 0.32724881172180176\n",
      "load_obj te_index started\n",
      "load_obj te_index: 0.10298418998718262\n",
      "load_obj daset_text started\n",
      "load_obj daset_text: 5.469642877578735\n"
     ]
    }
   ],
   "source": [
    "tr_index = load_obj('tr_index')\n",
    "te_index = load_obj('te_index')\n",
    "text = load_obj('daset_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stop_words.extend(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/text_tfidf0_feature_names.pkl'):\n",
    "    text_tfidf0 = TfidfVectorizer(max_features=50000, stop_words=stop_words)\n",
    "    with timeit('text_tfidf0 fit'):\n",
    "        text_tfidf0.fit(text.sample(frac=.7))\n",
    "    with timeit('text_tfidf0 transform'):\n",
    "        tr_text_tfidf0_wvs = text_tfidf0.transform(text.loc[tr_index])\n",
    "        te_text_tfidf0_wvs = text_tfidf0.transform(text.loc[te_index])\n",
    "    dump_obj(text_tfidf0.get_feature_names(), 'text_tfidf0_feature_names')\n",
    "    dump_obj(tr_text_tfidf0_wvs,'tr_text_tfidf0_wvs')\n",
    "    dump_obj(te_text_tfidf0_wvs,'te_text_tfidf0_wvs')\n",
    "\n",
    "    del text_tfidf0, tr_text_tfidf0_wvs, te_text_tfidf0_wvs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/text_tfidf1_feature_names.pkl'):\n",
    "    text_tfidf1 = TfidfVectorizer(max_features=50000, stop_words=stop_words, norm='l1', sublinear_tf=True)\n",
    "    with timeit('text_tfidf1 fit'):\n",
    "        text_tfidf1.fit(text.sample(frac=.7))\n",
    "\n",
    "    with timeit('text_tfidf1 transform'):\n",
    "        tr_text_tfidf1_wvs = text_tfidf1.transform(text.loc[tr_index])\n",
    "        te_text_tfidf1_wvs = text_tfidf1.transform(text.loc[te_index])\n",
    "\n",
    "    dump_obj(text_tfidf1.get_feature_names(), 'text_tfidf1_feature_names')\n",
    "    dump_obj(tr_text_tfidf1_wvs,'tr_text_tfidf1_wvs')\n",
    "    dump_obj(te_text_tfidf1_wvs,'te_text_tfidf1_wvs')\n",
    "\n",
    "    del text_tfidf1, tr_text_tfidf1_wvs, te_text_tfidf1_wvs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_obj emoji started\n",
      "load_obj emoji: 0.0005438327789306641\n"
     ]
    }
   ],
   "source": [
    "punct = set(string.punctuation)\n",
    "emoji = load_obj('emoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_1  = re.compile(\"\\d+х\\d+х\\d+\")\n",
    "rule_2  = re.compile(\"\\d+х\\d+\")\n",
    "rule_3  = re.compile(\"\\d*[-|–]\\d*\")\n",
    "rule_4  = re.compile(\"\\d*\\\\.\\d*\")\n",
    "rule_5  = re.compile(\"([^\\W\\d_]+)(\\d+)\")\n",
    "rule_6  = re.compile(\"(\\d+)([^\\W\\d_]+)\")\n",
    "rule_7  = re.compile(\"\\d+\\\\/\\d|\\d+-к|\\d+к|\\\\.\\/|\\d+х\\d+х\\d+|\\d+х\\d+\")\n",
    "rule_8  = re.compile(\"\\\\s+\")\n",
    "rule_9  = re.compile(\"([nn\\\\s]+)\")\n",
    "rule_10 = re.compile(\"\\d+nn\")\n",
    "rule_11 = re.compile(\"\\d+n\")\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = s.lower()\n",
    "    s = rule_1.sub('nxnxn ', s)\n",
    "    s = rule_2.sub('nxn ', s)\n",
    "    s = rule_3.sub('nn ', s)\n",
    "    s = rule_4.sub('n ', s)\n",
    "    s = rule_5.sub(lambda m: 'n' + m.group(1) + ' ', s)\n",
    "    s = rule_6.sub(lambda m: 'n' + m.group(2) + ' ', s)\n",
    "    s = rule_7.sub(' ', s)\n",
    "\n",
    "    s = ''.join([c if c.isalpha() or c.isalnum() or c.isspace() else ' ' for c in s \n",
    "                 if s not in emoji and s not in punct and not s.isnumeric()])\n",
    "    s = rule_8.sub(' ', s)\n",
    "    s = rule_9.sub('nn ', s)\n",
    "    s = rule_10.sub('nn ', s)\n",
    "    s = rule_11.sub('n ', s)\n",
    "    s = s.strip()\n",
    "    words = [w for w in s.split(' ') if w not in stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/normalized_text.pkl'):\n",
    "    with timeit('normalize_text'):\n",
    "        normalized_text = text.apply(normalize_text)\n",
    "        dump_obj(normalized_text,'normalized_text')\n",
    "        del normalized_text\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_obj normalized_text started\n",
      "load_obj normalized_text: 5.66666316986084\n",
      "0 самараnn детскаяnn одеждаnn иnn обувьnn дляnn мальчиковnn зимнийnn комбенизонnn зимнийnn комбенизонnn дляnn малыша\n",
      "1 братскnn телефоныnn аксессуарыnn чехолnn дляnn айфонаnn snn твёрдыйnn пластик\n",
      "2 краснокамскnn одеждаnn обувьnn аксессуарыnn женскаяnn одеждаnn продамnn кожаннуюnn курткуnn\n",
      "3 оренбургnn товарыnn дляnn животныхnn продамnn игрушкиnn шлейкуnn продамnn ценаnn указанаnn заnn всеnn чтоnn наnn фотоnn всеnn вnn идеальномnn состояниеnn еслиnn объявленияnn наnn сайтеnn значитnn актуальноnn\n",
      "4 ярославльnn одеждаnn обувьnn аксессуарыnn мужскаяnn одеждаnn спортивныйnn костюмnn adidasnn спортивныйnn костюмnn размерnn nn  nn  малоnn бnn уnn небольшойnn торгnn\n",
      "5 иркутскnn одеждаnn обувьnn аксессуарыnn женскаяnn одеждаnn плащnn\n",
      "6 устьnn ордынскийnn телефоныnn samsunn gnn samsuнгnn enn телефонnn экранnn разрешениеnn xnn безnn камерыnn безnn слотаnn дляnn картnn памятиnn аккумуляторnn nn  маnn чnn весnn nn  гnn шxвxтnn xnn xnn ммnn радио\n",
      "7 теплоеnn квартирыnn продамnn кnn квартираnn nn  м²nn этnn срочноnn всяnn информацияnn потелефону\n",
      "8 бузулукnn кошкиnn другаяnn отдамnn котёнкаnn отдамnn котёнкаnn мальчикnn nn  nn  месяцаnn вnn добрыеnn руки\n",
      "9 нижнетроицкийnn продуктыnn питанияnn баранинаnn мясоnn деревенскоеnn живойnn вес\n"
     ]
    }
   ],
   "source": [
    "normalized_text = load_obj('normalized_text')\n",
    "for i, s in enumerate(normalized_text[:10].values):\n",
    "    print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/text_tfidf2_feature_names.pkl'):\n",
    "    text_tfidf2 = TfidfVectorizer(max_features=50000)\n",
    "    with timeit('text_tfidf2 fit'):\n",
    "        text_tfidf2.fit(normalized_text.sample(frac=.7))\n",
    "    with timeit('text_tfidf2 transform'):\n",
    "        tr_text_tfidf2_wvs = text_tfidf2.transform(normalized_text.loc[tr_index])\n",
    "        te_text_tfidf2_wvs = text_tfidf2.transform(normalized_text.loc[te_index])\n",
    "    dump_obj(text_tfidf2.get_feature_names(), 'text_tfidf2_feature_names')\n",
    "    dump_obj(tr_text_tfidf2_wvs,'tr_text_tfidf2_wvs')\n",
    "    dump_obj(te_text_tfidf2_wvs,'te_text_tfidf2_wvs')\n",
    "\n",
    "    del text_tfidf2, tr_text_tfidf2_wvs, te_text_tfidf2_wvs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/text_tfidf3_feature_names.pkl'):\n",
    "    text_tfidf3 = TfidfVectorizer(max_features=50000, stop_words=stop_words, norm='l1', sublinear_tf=True)\n",
    "    with timeit('text_tfidf3 fit'):\n",
    "        text_tfidf3.fit(normalized_text.sample(frac=.7))\n",
    "    with timeit('text_tfidf3 transform'):\n",
    "        tr_text_tfidf3_wvs = text_tfidf3.transform(normalized_text.loc[tr_index])\n",
    "        te_text_tfidf3_wvs = text_tfidf3.transform(normalized_text.loc[te_index])\n",
    "    dump_obj(text_tfidf3.get_feature_names(), 'text_tfidf3_feature_names')\n",
    "    dump_obj(tr_text_tfidf3_wvs,'tr_text_tfidf3_wvs')\n",
    "    dump_obj(te_text_tfidf3_wvs,'te_text_tfidf3_wvs')\n",
    "\n",
    "    del text_tfidf3, tr_text_tfidf3_wvs, te_text_tfidf3_wvs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_obj daset_params started\n",
      "load_obj daset_params: 1.867748498916626\n"
     ]
    }
   ],
   "source": [
    "params = load_obj('daset_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "3b896605f03c    для мальчиков верхняя одежда 86-92 см (1-2 года)\n",
       "c81c34d3aac5                аксессуары чехлы и плёнки исключение\n",
       "a8ab6225dd06             женская одежда верхняя одежда 44–46 (m)\n",
       "20bf5eea7fb5                    исключение исключение исключение\n",
       "e1cf6b9d5789                    мужская одежда другое исключение\n",
       "Name: params, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/param_vec0_feature_names.pkl'):\n",
    "    param_cvec0 = CountVectorizer(max_features=1000, analyzer='word')\n",
    "    with timeit('param_cvec1 fit'):\n",
    "        param_cvec0.fit(params.sample(frac=.7))\n",
    "    with timeit('param_cvec1 transform'):\n",
    "        tr_param_cvec0_wvs = param_cvec0.transform(params.loc[tr_index])\n",
    "        te_param_cvec0_wvs = param_cvec0.transform(params.loc[te_index])\n",
    "    dump_obj(param_cvec0.get_feature_names(), 'param_vec0_feature_names')\n",
    "    dump_obj(tr_param_cvec0_wvs,'tr_param_cvec0_wvs')\n",
    "    dump_obj(te_param_cvec0_wvs,'te_param_cvec0_wvs')\n",
    "\n",
    "    del param_cvec0, tr_param_cvec0_wvs, te_param_cvec0_wvs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'features/param_vec1_feature_names.pkl'):\n",
    "    param_cvec1 = CountVectorizer(max_features=80, analyzer='char')\n",
    "    with timeit('param_cvec1 fit'):\n",
    "        param_cvec1.fit(params.sample(frac=.7))\n",
    "    with timeit('param_cvec1 transform'):\n",
    "        tr_param_cvec1_cvs = param_cvec1.transform(params.loc[tr_index])\n",
    "        te_param_cvec1_cvs = param_cvec1.transform(params.loc[te_index])\n",
    "    dump_obj(param_cvec1.get_feature_names(), 'param_vec1_feature_names')\n",
    "    dump_obj(tr_param_cvec1_cvs,'tr_param_cvec1_cvs')\n",
    "    dump_obj(te_param_cvec1_cvs,'te_param_cvec1_cvs')\n",
    "\n",
    "    del param_cvec1, tr_param_cvec1_cvs, te_param_cvec1_cvs\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
