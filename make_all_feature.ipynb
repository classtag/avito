{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "extract features\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import copy\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import gc\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/kaggle/competitions/avito-demand-prediction/'\n",
    "\n",
    "def dump_matrix(matrix, name):\n",
    "    pickle.dump(matrix, open(root + 'features/{}.pkl'.format(name), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "def load_matrix(name):\n",
    "    return pickle.load(open(root + 'features/{}.pkl'.format(name), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['region', 'parent_category_name', 'user_type', 'user_id']\n",
    "num_cols = ['price', 'image_top_1', 'item_seq_number']\n",
    "txt_cols = ['city', 'category_name', 'param_1', 'title', 'description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "\n",
    "tr = pd.read_csv(root+'train.csv.zip', parse_dates=['activation_date'])  # 1503424\n",
    "te = pd.read_csv(root+'test.csv.zip',  parse_dates=['activation_date'])  # 508438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.sort_values('activation_date', inplace=True)\n",
    "te.sort_values('activation_date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train date: 2017-04-07 00:00:00\n",
      "Min test date: 2017-04-12 00:00:00\n",
      "Data shape is (1503424, 18) (508438, 17)\n"
     ]
    }
   ],
   "source": [
    "print('Max train date:', tr.activation_date.max())\n",
    "print('Min test date:', te.activation_date.min())\n",
    "print('Data shape is', tr.shape, te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['n_missing_features'] = tr.drop('deal_probability', axis=1).isnull().sum(axis=1)\n",
    "te['n_missing_features'] = te.isnull().sum(axis=1)\n",
    "num_cols.append('n_missing_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = pd.read_csv(root+'aggregated_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr.merge(gp, on='user_id', how='left')\n",
    "te = te.merge(gp, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols += list(gp.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tr.deal_probability\n",
    "tr_index = tr['item_id']\n",
    "te_index = te['item_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_matrix(y, 'y')\n",
    "dump_matrix(tr_index,'tr_index')\n",
    "dump_matrix(te_index,'te_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daset shape rows:2011862 cols:21\n"
     ]
    }
   ],
   "source": [
    "daset = pd.concat([tr, te], axis=0)\n",
    "daset.set_index('item_id', inplace=True)\n",
    "print('Daset shape rows:{} cols:{}'.format(*daset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tr, te, daset['image']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//////////////////////////////////////////////////\n",
      "=====activation_date value coutns======\n",
      "2017-03-15    108615\n",
      "2017-03-16    106168\n",
      "2017-03-17     98773\n",
      "2017-03-18     97554\n",
      "2017-03-19    114416\n",
      "2017-03-20    115190\n",
      "2017-03-21    110535\n",
      "2017-03-22    109813\n",
      "2017-03-23    106544\n",
      "2017-03-24     97351\n",
      "2017-03-25     97104\n",
      "2017-03-26    113513\n",
      "2017-03-27    114863\n",
      "2017-03-28    112885\n",
      "2017-03-29        87\n",
      "2017-03-30         3\n",
      "2017-03-31         1\n",
      "2017-04-01         3\n",
      "2017-04-02         3\n",
      "2017-04-03         2\n",
      "2017-04-07         1\n",
      "2017-04-12     81824\n",
      "2017-04-13     77176\n",
      "2017-04-14     70366\n",
      "2017-04-15     58793\n",
      "2017-04-16     58909\n",
      "2017-04-17     80191\n",
      "2017-04-18     81114\n",
      "2017-04-19        64\n",
      "2017-04-20         1\n",
      "Name: activation_date, dtype: int64\n",
      "//////////////////////////////////////////////////\n"
     ]
    }
   ],
   "source": [
    "print(50 * '/')\n",
    "print('=====activation_date value coutns======')\n",
    "print(daset.activation_date.value_counts().sort_index())\n",
    "print(50 * '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing .... \n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing .... ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "daset['day_of_week'] = daset.activation_date.dt.weekday\n",
    "cat_cols.append('day_of_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base categorical feature\n",
      "Handle missing\n"
     ]
    }
   ],
   "source": [
    "print('Base categorical feature')\n",
    "for c in ['region', 'parent_category_name', 'user_type', 'day_of_week']:\n",
    "    d = daset[c].value_counts().to_dict()\n",
    "    daset[c + '_factor'] = daset[c].apply(lambda x: d.get(x, 0)).astype(int)\n",
    "    num_cols.append(c + '_factor')\n",
    "\n",
    "print('Handle missing')\n",
    "daset[\"price\"] = np.log1p(daset[\"price\"]) + 0.001\n",
    "daset[\"price\"].fillna(-1, inplace=True)\n",
    "daset[\"image_top_1\"] = daset[\"image_top_1\"].fillna(-1).astype(int)\n",
    "daset[\"item_seq_number\"] = daset[\"item_seq_number\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_encode ...\n",
      "target enc region\n",
      "price enc region\n",
      "target enc image_top_1\n",
      "price enc image_top_1\n",
      "target enc category_name\n",
      "price enc category_name\n",
      "target enc user_type\n",
      "price enc user_type\n",
      "target enc parent_category_name\n",
      "price enc parent_category_name\n",
      "target enc day_of_week\n",
      "price enc day_of_week\n"
     ]
    }
   ],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "\n",
    "print('target_encode ...')\n",
    "target_enc_cols = [\n",
    "    'region', 'image_top_1', 'category_name', 'user_type',\n",
    "    'parent_category_name','day_of_week'\n",
    "]\n",
    "for c in target_enc_cols:\n",
    "    trn_series = daset.loc[tr_index, c].fillna('nan')\n",
    "    tst_series = daset.loc[te_index, c].fillna('nan')\n",
    "    print('target enc {}'.format(c))\n",
    "    tr_avg, te_avg = target_encode(trn_series,tst_series, y)\n",
    "    cc = 'n_' + c + '_target_avg'\n",
    "    daset.loc[tr_index, cc] = tr_avg.values\n",
    "    daset.loc[te_index, cc] = te_avg.values\n",
    "    num_cols.append(cc)\n",
    "    \n",
    "    print('price enc {}'.format(c))\n",
    "    tr_avg, te_avg = target_encode(trn_series,tst_series, daset.loc[tr_index,'price'])\n",
    "    cc = 'n_' + c + '_price_avg'\n",
    "    daset.loc[tr_index, cc] = tr_avg.values\n",
    "    daset.loc[te_index, cc] = te_avg.values\n",
    "    num_cols.append(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add uv\n"
     ]
    }
   ],
   "source": [
    "print('Add uv')\n",
    "for c in ['region', 'parent_category_name', 'user_type', 'city','day_of_week',\n",
    "          'category_name', 'image_top_1', 'item_seq_number']:\n",
    "    pv = daset[c].value_counts()\n",
    "    uv = daset.user_id.groupby(daset[c]).agg(lambda x: len(np.unique(x)))\n",
    "    if len(pv) > 1000:\n",
    "        pv = pv[:1000]\n",
    "        uv = uv[:1000]\n",
    "\n",
    "    upv = pv / uv\n",
    "    daset[c + '_uv'] = daset[c].map(uv).fillna(-1).astype(int)\n",
    "    daset[c + '_upv'] = daset[c].map(upv).fillna(-1).astype(float)\n",
    "    num_cols.append(c + '_uv')\n",
    "    num_cols.append(c + '_upv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_w2v(sentences, model, num_features):\n",
    "    def _average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        n_words = 0.\n",
    "        for word in words:\n",
    "            if word in vocabulary:\n",
    "                n_words = n_words + 1.\n",
    "                feature_vector = np.add(feature_vector, model[word])\n",
    "\n",
    "        if n_words:\n",
    "            feature_vector = np.divide(feature_vector, n_words)\n",
    "        return feature_vector\n",
    "\n",
    "    vocab = set(model.wv.index2word)\n",
    "    feats = [_average_word_vectors(s, model, vocab, num_features) for s in sentences]\n",
    "    return csr_matrix(np.array(feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_target_cluster\n"
     ]
    }
   ],
   "source": [
    "print('add_target_cluster')\n",
    "\n",
    "\n",
    "def add_target_cluster(c):\n",
    "    print('add_target_cluster {}'.format(c))\n",
    "    gkey = c\n",
    "    _nan = daset.loc[daset[c].isnull()].shape[0] > 0\n",
    "    if _nan:\n",
    "        gkey = c + '_tmp'\n",
    "        daset[gkey] = daset[c].fillna('NAN')\n",
    "    gp = daset.loc[tr_index, [gkey,'deal_probability']].groupby(gkey)['deal_probability']\n",
    "    hist = gp.agg(lambda x: ' '.join(x.apply(lambda i: round(i, 2)).astype(str)))\n",
    "    gp_index = hist.index\n",
    "    sentences = [x.split(' ') for x in hist.values]\n",
    "    n_features = 500\n",
    "    w2v = Word2Vec(sentences=sentences, min_count=1, size=n_features)\n",
    "    w2v_feature = apply_w2v(sentences, model=w2v, num_features=n_features)\n",
    "\n",
    "    del sentences, hist\n",
    "    gc.collect()\n",
    "\n",
    "    # clustering\n",
    "    sims = cosine_similarity(w2v_feature)\n",
    "    Z = linkage(sims, 'ward')\n",
    "    max_dist = 1.0\n",
    "    cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
    "    cluster_labels = pd.Series(cluster_labels, name=c + '_cluster', index=gp_index)\n",
    "    daset[c + '_cluster'] = daset[gkey].map(cluster_labels).fillna(-1).astype(int)\n",
    "    num_cols.append(c + '_cluster')\n",
    "    if _nan:\n",
    "        del daset[gkey]\n",
    "    del sims, Z, cluster_labels\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add_target_cluster image_top_1\n",
      "image_top_1 471\n",
      "add_target_cluster param_1\n",
      "param_1 130\n",
      "add_target_cluster category_name\n",
      "category_name 15\n",
      "add_target_cluster city\n",
      "city 551\n"
     ]
    }
   ],
   "source": [
    "for c in ['image_top_1', 'param_1', 'category_name', 'city']:\n",
    "    add_target_cluster(c)\n",
    "    print(c, len(daset[c+'_cluster'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat2Vec...\n",
      "fit_cat2vec_model\n",
      "apply_w2v for cat2vec\n"
     ]
    }
   ],
   "source": [
    "print('Cat2Vec...')\n",
    "n_cat2vec_feature = 10\n",
    "n_cat2vec_window = 5\n",
    "\n",
    "\n",
    "def gen_cat2vec_sentences(data):\n",
    "    X_w2v = copy.deepcopy(data)\n",
    "    names = list(X_w2v.columns.values)\n",
    "    for c in names:\n",
    "        X_w2v[c] = X_w2v[c].fillna('nan').astype('category')\n",
    "        X_w2v[c].cat.categories = [\"%s:%s\" % (c, g) for g in X_w2v[c].cat.categories]\n",
    "    X_w2v = X_w2v.values.tolist()\n",
    "    return X_w2v\n",
    "\n",
    "\n",
    "def fit_cat2vec_model():\n",
    "    X_w2v = gen_cat2vec_sentences(daset.loc[:, cat_cols].sample(frac=0.8))\n",
    "    for i in X_w2v:\n",
    "        shuffle(i)\n",
    "    model = Word2Vec(X_w2v, size=n_cat2vec_feature, window=n_cat2vec_window)\n",
    "    return model\n",
    "\n",
    "print('fit_cat2vec_model')\n",
    "c2v_model = fit_cat2vec_model()\n",
    "print('apply_w2v for cat2vec')\n",
    "tr_c2v_matrix = apply_w2v(gen_cat2vec_sentences(daset.loc[tr_index, cat_cols]), c2v_model, n_cat2vec_feature)\n",
    "te_c2v_matrix = apply_w2v(gen_cat2vec_sentences(daset.loc[te_index, cat_cols]), c2v_model, n_cat2vec_feature)\n",
    "dump_matrix(tr_c2v_matrix, 'tr_c2v_matrix')\n",
    "dump_matrix(te_c2v_matrix, 'te_c2v_matrix')\n",
    "del tr_c2v_matrix, te_c2v_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji = set()\n",
    "for s in daset['title'].fillna('').astype(str):\n",
    "    for c in s:\n",
    "        if c.isdigit() or c.isalpha() or c.isalnum() or c.isspace() or c in punct:\n",
    "            continue\n",
    "        emoji.add(c)\n",
    "\n",
    "for s in daset['description'].fillna('').astype(str):\n",
    "    for c in str(s):\n",
    "        if c.isdigit() or c.isalpha() or c.isalnum() or c.isspace() or c in punct:\n",
    "            continue\n",
    "        emoji.add(c)\n",
    "        \n",
    "dump_matrix(emoji,'emoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "daset['n_titl_len'] = daset['title'].fillna('').apply(len)\n",
    "daset['n_titl_wds'] = daset['title'].fillna('').apply(lambda x: len(x.split(' ')))\n",
    "daset['n_titl_dig'] = daset['title'].fillna('').apply(lambda x: sum(c.isdigit() for c in x))\n",
    "daset['n_titl_cap'] = daset['title'].fillna('').apply(lambda x: sum(c.isupper() for c in x))\n",
    "daset['n_titl_spa'] = daset['title'].fillna('').apply(lambda x: sum(c.isspace() for c in x))\n",
    "daset['n_titl_pun'] = daset['title'].fillna('').apply(lambda x: sum(c in punct for c in x))\n",
    "daset['n_titl_emo'] = daset['title'].fillna('').apply(lambda x: sum(c in emoji for c in x))\n",
    "\n",
    "daset['r_titl_wds'] = daset['n_titl_wds']/(daset['n_titl_len']+1)\n",
    "daset['r_titl_dig'] = daset['n_titl_dig']/(daset['n_titl_len']+1)\n",
    "daset['r_titl_cap'] = daset['n_titl_cap']/(daset['n_titl_len']+1)\n",
    "daset['r_titl_spa'] = daset['n_titl_spa']/(daset['n_titl_len']+1)\n",
    "daset['r_titl_pun'] = daset['n_titl_pun']/(daset['n_titl_len']+1)\n",
    "daset['r_titl_emo'] = daset['n_titl_emo']/(daset['n_titl_len']+1)\n",
    "\n",
    "daset['n_desc_len'] = daset['description'].fillna('').apply(len)\n",
    "daset['n_desc_wds'] = daset['description'].fillna('').apply(lambda x: len(x.split(' ')))\n",
    "daset['n_desc_dig'] = daset['description'].fillna('').apply(lambda x: sum(c in punct for c in x))\n",
    "daset['n_desc_cap'] = daset['description'].fillna('').apply(lambda x: sum(c.isdigit() for c in x))\n",
    "daset['n_desc_pun'] = daset['description'].fillna('').apply(lambda x: sum(c.isupper() for c in x))\n",
    "daset['n_desc_spa'] = daset['description'].fillna('').apply(lambda x: sum(c.isspace() for c in x))\n",
    "daset['n_desc_emo'] = daset['description'].fillna('').apply(lambda x: sum(c in emoji for c in x))\n",
    "daset['n_desc_row'] = daset['description'].astype(str).apply(lambda x: x.count('/\\n'))\n",
    "\n",
    "daset['r_desc_wds'] = daset['n_desc_wds']/(daset['n_desc_len']+1)\n",
    "daset['r_desc_dig'] = daset['n_desc_dig']/(daset['n_desc_len']+1)\n",
    "daset['r_desc_cap'] = daset['n_desc_cap']/(daset['n_desc_len']+1)\n",
    "daset['r_desc_spa'] = daset['n_desc_spa']/(daset['n_desc_len']+1)\n",
    "daset['r_desc_pun'] = daset['n_desc_pun']/(daset['n_desc_len']+1)\n",
    "daset['r_desc_row'] = daset['n_desc_row']/(daset['n_desc_len']+1)\n",
    "daset['r_desc_emo'] = daset['n_desc_emo']/(daset['n_desc_len']+1)\n",
    "\n",
    "daset['r_titl_des'] = daset['n_titl_len']/(daset['n_desc_len']+1)\n",
    "\n",
    "num_cols += list(daset.filter(regex='_titl_|_desc_').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_param(x):\n",
    "    text = ' '.join(x)\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parmas...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('parmas...')\n",
    "pms_cols = ['param_1', 'param_2', 'param_3']\n",
    "daset['params'] = daset[pms_cols].fillna('исключение').apply(cat_param, axis=1)\n",
    "del daset['param_2'], daset['param_3']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "daset['n_char_params'] = daset.params.apply(len)\n",
    "num_cols.append('n_char_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params Model CountVectorizer\n",
      "transform params to vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pms_tv = CountVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 6),\n",
    "    strip_accents='unicode')\n",
    "\n",
    "print('Params Model CountVectorizer')\n",
    "pms_tv.fit(daset['params'].sample(frac=0.7))\n",
    "print('transform params to vector')\n",
    "tr_param_vec_matrix = pms_tv.transform(daset.loc[tr_index, 'params'])\n",
    "te_param_vec_matrix = pms_tv.transform(daset.loc[te_index, 'params'])\n",
    "\n",
    "dump_matrix(pms_tv, 'pms_tv')\n",
    "dump_matrix(pms_tv.get_feature_names(), 'pms_tv_feature_names')\n",
    "dump_matrix(daset['params'], 'daset_params')\n",
    "dump_matrix(tr_param_vec_matrix, 'tr_param_vec_matrix')\n",
    "dump_matrix(te_param_vec_matrix, 'te_param_vec_matrix')\n",
    "del tr_param_vec_matrix, te_param_vec_matrix, daset['params']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ...\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Text generating\n"
     ]
    }
   ],
   "source": [
    "print('text ...')\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "stop_words.extend(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def text_mask(x):\n",
    "    return x not in set(punctuation) and x not in stop_words\n",
    "\n",
    "def cat_text(s):\n",
    "    s = ' '.join(s)\n",
    "    return s\n",
    "\n",
    "print('Text generating')\n",
    "daset['text'] = daset[txt_cols].fillna('').apply(cat_text, axis=1)\n",
    "daset['n_text_wds'] = daset['text'].apply(lambda x: len(x.split(' ')))\n",
    "num_cols.append('n_text_wds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daset['text'] = load_matrix('daset_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Model CountVectorizer\n",
      "transform text to vector\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daset.drop(txt_cols, axis=1, inplace=True)\n",
    "gc.collect()\n",
    "\n",
    "txt_tv = CountVectorizer(\n",
    "    max_features=50000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stop_words,\n",
    "    analyzer='word')\n",
    "\n",
    "print('Text Model CountVectorizer')\n",
    "txt_tv.fit(daset['text'].sample(frac=0.7))\n",
    "print('transform text to vector')\n",
    "tr_txt_vec_matrix = txt_tv.transform(daset.loc[tr_index, 'text'])\n",
    "te_txt_vec_matrix = txt_tv.transform(daset.loc[te_index, 'text'])\n",
    "\n",
    "dump_matrix(txt_tv, 'txt_tv')\n",
    "dump_matrix(txt_tv.get_feature_names(), 'txt_tv_feature_names')\n",
    "dump_matrix(daset['text'], 'daset_text')\n",
    "dump_matrix(tr_txt_vec_matrix, 'tr_txt_vec_matrix')\n",
    "dump_matrix(te_txt_vec_matrix, 'te_txt_vec_matrix')\n",
    "del tr_txt_vec_matrix, te_txt_vec_matrix, daset['text']\n",
    "# daset.drop(cat_cols, axis=1, inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_matrix(daset.loc[:, num_cols], 'daset_num_cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = num_cols \\\n",
    "    + ['c2v_%d' % i for i in range(n_cat2vec_feature)] \\\n",
    "    + ['p2v_%s' % c for c in pms_tv.get_feature_names()] \\\n",
    "    + ['t2v_%s' % c for c in txt_tv.get_feature_names()]\n",
    "dump_matrix(feature_names, 'feature_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cat2vec_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
